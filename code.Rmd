---
title: "Commented Code from Talk"
author: "Julian Quandt"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float: true
    number_sections: true
    fig_width: 6
    fig_height: 4
    fig_caption: true
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# First simple simulation

## Create a design matrix

For each simulation, the first step we want to take care of is to create a design matrix.
In simple terms, this is a data frame that contains the information about parameter values as well as group identities and other info of participants.
In this case, the design matrix needs to contain the following information:

-   `group`: The group identity of the participant (i.e., negative vs. positive clip)
-   `intercept (b0)`: the value we assign to the mean of the negative group
-   `slope (b1)`: the value we assign to the difference between the negative and positive group
-   `error`: the error term that we add to the data

```{r}
set.seed(234)

n_participants <- 100
group <- rep(c(0, 1), each = n_participants / 2)

b0 <- 0.5
b1 <- 0.2
error <- rnorm(n_participants, 0, 1)

design_matrix <- data.frame(b0, b1, group, error)

View(design_matrix)
```

## Calculate the dependent variable

Next, we want to create a dependent variable that is based on the design matrix.
In this case, we want to create a dependent variable that is based on the following formula:

$$
y = b0 + b1 * group + error
$$

```{r}
design_matrix$Y <- design_matrix$b0 + design_matrix$b1 * design_matrix$group + design_matrix$error
View(design_matrix)
```

In principle, we could no just run a t-test on the dependent variable to see if the groups differ.

```{r}
t.test(design_matrix$Y ~ design_matrix$group, var.equal = TRUE)
```

To illustrate the point, we can see that this is exactly the same as running a linear model.

```{r}
summary(lm(Y ~ group, data = design_matrix))
```

As we can see, the p-values of the t-test and the linear model are identical.

If we take a close look at parameter values, however, we might wonder why the parameter for the group effect is not 0.2 as we wanted but 0.33.

This is due to the fact that there is noise in the simulation and with only 100 people, the simulated scores might not be close to the parameter value that we intended to simulate.
If we increase the number of participants, we can see that the parameter estimates get closer to the true value.

```{r}
set.seed(345)

n_participants <- 100000

error <- rnorm(n_participants, 0, 1)

group <- rep(c(0, 1), each = n_participants / 2)

design_matrix <- data.frame(b0,b1, group, error)

design_matrix$Y <- design_matrix$b0 + design_matrix$b1 * design_matrix$group + design_matrix$error
```

```{r}
summary(lm(Y ~ group, data = design_matrix))
```

In this case, the parameter estimates are much closer to the intended value of 0.2.

## Simulate this question as a pre-post design

Now, let's simulate a pre-post design for the same question.
This time, our dependent variable will be the post scores, where the pre scores are added to the model as a predictor.

This time the model formula is:

$$
post opinion = pre opinion + groupeffect + error
$$

In this case, as we want to simulate a pre-post design, in which the group effect depends on the group, we will precalculate the b1\*group value and add it to the dependent variable.

Note that this is technically identical to just using b1 = 0.2 but one nice thing of simulations is that we don't need to think about these technicalities, and can instead directly put in the numbers that we thought of.

```{r}
set.seed(345)

n_participants <- 100
pre_measure <- floor(rnorm(n_participants, 5, 2))
pre_measure
pre_measure <- ifelse(pre_measure < 1, 1, pre_measure)
pre_measure <- ifelse(pre_measure > 7, 7, pre_measure)
pre_measure

error <- rnorm(n_participants, 0, 1)

group <- rep(c(0, 1), each = n_participants / 2)

design_matrix <- data.frame(pre_measure, group, error)
design_matrix$group_effect <- ifelse(design_matrix$group == 1, 0.5+0.2, 0.5)

design_matrix$Y <- design_matrix$pre_measure + design_matrix$group_effect + design_matrix$error
```

Now we can test this with ANCOVA

```{r}
library(car)
ancova_model <- aov(Y ~ pre_measure + group, data = design_matrix)
Anova(ancova_model, type="III")

```

Now, we can run a linear model to see if the groups differ.

```{r}
summary(lm(Y ~ pre_measure + group , data = design_matrix))
```

As we can see, the test statistics are again identical.

Specifically, the squared t-values of the lm are identical to the F-values of the ancova.

## Short example of power simulation

So if we want to know what our power with e.g. 100 participants would be, we can repeat the simulation many times, save the p-values for each simulation, and then calculate the proportion of times that the p-value is below 0.05.

```{r}
set.seed(123)
n_participants <- 100
b0 <- 0.5
b1 <- 0.2

p_values <- c()

for(i in 1:1000){

  n_participants <- 100

  error <- rnorm(n_participants, 0, 1)

  group <- rep(c(0, 1), each = n_participants / 2)

  design_matrix <- data.frame(b0, b1, group, error)

  design_matrix$Y <- design_matrix$b0 + design_matrix$b1 * design_matrix$group + design_matrix$error

  simulated_p_value <- t.test(design_matrix$Y ~ design_matrix$group, var.equal = TRUE)$p.value

  p_values <- c(p_values, simulated_p_value)

}

power <- mean(p_values < 0.05)
print(power)
```

In this case, the power of the test would be 0.173.

So let's compare this to G\*Power.

![The same power analysis in G\*Power](src/g_power_ttest.png)

As we can see, the power is very similar to the power we calculated in R.

# Simulating mixed models

To make this easier for us, we will write a function that generates the design matrix for us.
This is basically doing what we did before, but in a function that we can call with different parameters like the number of participants, the number of genres, and the number of songs.

```{r}
generate_design <- function(n_participants, n_genres, n_songs){
  
  design_matrix <- expand.grid(participant = 1:n_participants, genre = 1:n_genres, song = 1:n_songs) # adding song to expand.grid
  design_matrix$genre <- ifelse(design_matrix$genre ==1, "rock", "pop")
  design_matrix$genre <- factor(design_matrix$genre)
  # make rock the reference level, so it gets 1 in the contrast matrix and pop gets -1
  design_matrix$genre <- relevel(design_matrix$genre, ref = "rock")
  contrasts(design_matrix$genre) <- contr.sum(2)
  design_matrix$song <- paste0(design_matrix$genre, "_", design_matrix$song) 
  return(design_matrix)
}
```

```{r}
n_participants <- 10
n_genres <- 2
n_songs <- 20

song_data <- generate_design(n_participants = n_participants, n_genres = n_genres, n_songs = n_songs)
View(song_data)
```

Now in the next step, we need to define the parameters again.
First lets start out with the parameters we had before:

```{r}
b0 <- 75 # this is what we think the overall liking is across genres in the population
b1 <- 5 # this is the difference between rock and pop songs, meaning that people on average like rock songs 5 points more than pop songs

```

As before, b0 and b1 are just single values, as they are *fixed effects* meaning that they are the same for each participant.

In contrast, the random effect for participants that we want to add is a bit more complicated.
In this case, we want to simulate that each participant has a different intercept.
This intercept reflects that each participant might have a different baseline level of liking for songs, that deviates somewhat from the 75 liking that we have in the population.
In other words, some people might like music a little bit less than others (lets say 60 on average) and others might like music a little bit more (lets say 80 on average).

```{r}
set.seed(123)

sd_u0 <- 7 # this is the standard deviation of the random intercepts
U0 <- rnorm(n_participants, 0, sd_u0) # this is the random intercept for each participant
print(U0)

hist(75+U0, main = "Participant-specific music liking", xlab = "Liking score (0-100)")
```

We can see in the histogram, that we now have different "baseline music liking" for each participant.

Now lets simulate the dependent variable based on this.

```{r}
set.seed(123)

sd_error <- 10 # this is the standard deviation of the error term aka residual variance
error <- rnorm(nrow(song_data), 0, sd_error) 

song_data1 <- song_data

for(i in 1:nrow(song_data1)){
  u0 <- U0[song_data1$participant[i]]
  x <- ifelse(song_data1$genre[i] == "rock", 0.5, -0.5)
  song_data1$Y[i] <- b0 + u0 + b1 * x + error[i]
}
View(song_data1)
```

TODO: change slide terms from treatment effect to example-specific terms (genre, song etc.)

In the above, we defined an error term that is again different for each observation.
This is because, each single observation has some unknown factors that influence it.
Hence, we need to have as many values for this error as we have observations.
For participants, we only have 10 values, as we only have 10 participants.

Now we can fit a mixed model to this data.

```{r}
library(lme4)
song_model_1 <- lmer(Y ~ genre + (1|participant), data = song_data1)
summary(song_model_1)
```

We can see that the model estimates are very close to the values that we simulated.
Note that the fixed effect is half the size of what we simulated.
This is correct, given that we use contrast coding, meaning that the estimate indicates the difference of each group from the grand mean (i.e. the mean of both groups combined).
As we wanted our intercept to represent overall liking for music across genres, this is the correct way to do this.
We can check this with the emmeans package:

```{r}
library(emmeans)
emmeans(song_model_1, "genre")
```

We see that the difference between the genres is 5 points, as we wanted.

## Convergence Warning: boundary (singular) fit: see ?isSingular

If you have fitted a mixed model before, you might have seen this warning.
In fact let's see what happens if we assume that participants liking for music is overall very similar to the population average.
In other words, what happens if participants do not differ in their baseline liking for music.

```{r}
set.seed(123)
sd_u0 <- 0.1 # this is the standard deviation of the random intercepts

U0 <- rnorm(n_participants, 0, sd_u0) # this is the random intercept for each participant

```

Now we can simulate the data again.

```{r}
set.seed(123)
song_data2 <- song_data
for(i in 1:nrow(song_data2)){

  u0 <- U0[song_data2$participant[i]]
  x <- ifelse(song_data2$genre[i] == "rock", 0.5, -0.5)

  song_data2$Y[i] <- b0 + u0 + b1 * x + error[i]
}
```

and again fit the model

```{r}
song_model_2 <- lmer(Y ~ genre + (1|participant), data = song_data2)
summary(song_model_2)
```

We can see that the we now get a warning that the model is singular.
This means that the best fitting model is one that is on the boundary of what the model can estimate.
The reason for this is that the deviation for overall liking of music across participants is estimated as a standard deviation.
By definition, standard deviations need to be greater than 0.
If we set the standard deviation to 0 or very close to 0, this warning will tell us that the best-fitting solution of the model is one that is very close to the boundary of what the model can estimate.

We can see that the other estimates of the model are nearly identical.
Hence, the other estimates can in this case still be trusted, and the warning only occurs because the model (rightfully) estimates the standard deviation of the random intercept to be very close to 0.

The problem is that usually, with real data, we do not *know* the true value of the random effects and hence, we cannot know for sure whether the singularity warning is trivial (like in this case) or not.
However, if we see that one random effect is estimated to be very close to 0, we can see what happens if we exclude this effect.

```{r}
song_model_3 <- lm(Y ~ genre, data = song_data2)
summary(song_model_3)
```

We see that the effects without the random intercept are very similar to the effects with the random intercept.

Now we can also see what happens if there *is* a participant-specific difference in overall liking, but we fail to account for it:

```{r}

song_model_4 <- lm(Y ~ genre, data = song_data1)
summary(song_model_4)
```

We see that in the latter case, when we just run a model without accounting for the random intercept, the p-value for the genre effect is larger, and the error variance is larger.
This is because the model will try to account for the participant-specific differences in overall liking by attributing this to the error term.
This in turn decreases the power of the model to detect the genre effect, and decreases the amount of explained variance.

## Singular fit because of too few observations

Another reason why a model does not converge, is that there might not be enough observations to distinguish different causes of variation in the data, for example the difference between the u0 term and the error term.

Let's see what happens if each person only listens to 1 song per genre:

```{r}

n_songs_single <- 1
n_participants_single <- 1000
song_data_single <- generate_design(n_participants = n_participants_single, n_genres = n_genres, n_songs = n_songs_single)

```

We can now simulate data with this new design matrix:

```{r}

set.seed(123)

error_single <- rnorm(nrow(song_data_single), 0, 10)

for(i in 1:nrow(song_data_single)){

  u0 <- U0[song_data_single$participant[i]]
  x <- ifelse(song_data_single$genre[i] == "rock", 0.5, -0.5)

  song_data_single$Y[i] <- b0 + u0 + b1 * x + error_single[i]
}
```

and fit the model

```{r}
song_model_single <- lmer(Y ~ genre + (1|participant), data = song_data_single)
summary(song_model_single)
```

In this case, we see that the model estimates the random intercept to be 0.
This is because there is no way for the model to tell whether variation is caused by the random intercept or the error term, as there is only one observation per participant and genre.

## Simulating Random Slopes

If we want to add a participant-specific adjustment to the effect of genre on their overall liking, we can do so by adding a random slope.

```{r}
set.seed(123)

sd_u0 <- 7 # this is the standard deviation of the random intercepts
U0 <- rnorm(n_participants, 0, sd_u0) # this is the random intercept for each participant

sd_u1 <- 3.5 # this is the standard deviation of the random slopes
U1 <- rnorm(n_participants, 0, sd_u1) # this is the set of random slopes for each participant
hist(5+U1, main = "Participant-specific genre effect", xlab = "Liking for Rock over Pop")
```

We can see in the histogram, that for some people the difference is now very strongly positive, indicating that they LOVE rock way more than pop, while for others the difference is very strongly negative, indicating that they LOVE pop way more than rock.

Now we can simulate the data again.

```{r}

song_data3 <- song_data
for(i in 1:nrow(song_data3)){
  u0 <- U0[song_data3$participant[i]]
  u1 <- U1[song_data3$participant[i]]
  x <- ifelse(song_data3$genre[i] == "rock", 0.5, -0.5)

  song_data3$Y[i] <- b0 + u0 + (b1 + u1) * x + error[i]
}

```

and fit the model

```{r}
song_model_5 <- lmer(Y ~ genre + (1+genre|participant), data = song_data3)
summary(song_model_5)
```

We can again see that the simulation worked out, with the new random slope estimate being half the size that we indicated, again as it is a contrast estimate.

```{r}
emmeans(song_model_5, "genre")
```

## Adding crossed random effects for songs

If we want to add a random effect for songs, we can do so by adding a random effect for songs that is independent of the random effect for participants.

```{r}
set.seed(123)

sd_w0 <- 15
W0 <- rnorm(n_songs*2, 0, sd_w0) #n_songs*2 as n_songs is the number of songs per genre


hist(75+W0, main = "Song-specific music liking", xlab = "Liking score (0-100)")
```

The histogram shows that songs differ hugely in how much they are liked overall in this new model.

Now we can simulate the data again.

```{r}
song_data4 <- song_data

for(i in 1:nrow(song_data4)){

  u0 <- U0[song_data4$participant[i]]
  u1 <- U1[song_data4$participant[i]]
  w0 <- W0[which(unique(song_data4$song) == song_data4$song[i])]
  x <- ifelse(song_data4$genre[i] == "rock", 0.5, -0.5)

  song_data4$Y[i] <- b0  + u0  + w0  + (b1 + u1) * x + error[i]
}
```

and fit the model

```{r}
song_model_6 <- lmer(Y ~ genre + (1+genre|participant) + (1|song), data = song_data4)
summary(song_model_6)
```

We can see, that the model does a decent job at recovering the estimates.
However, now the fixed effect of genre (supposed to be 2.5) is closer to 4, which seems a little odd.
The reason for this is that with this small sample size with only 10 people, making the estimation of the parameters more difficult.

Let's see what happens if we increase sample size to 100 people listening to 80 songs each:

```{r}
n_participants_large <- 100
n_genres <- 2
n_songs <- 80

song_data_large <- generate_design(n_participants = n_participants_large, n_genres = n_genres, n_songs = n_songs)
```

Lets again create random effects for participants and songs in this larger sample

```{r}
song_data5 <- song_data_large

set.seed(999)
U0_large <- rnorm(n_participants_large, 0, sd_u0)
U1_large <- rnorm(n_participants_large, 0, sd_u1)
W0_large <- rnorm(length(unique(song_data5$song)), 0, sd_w0) 
error_large <- rnorm(nrow(song_data5), 0, sd_error)
```

now lets simulate the data again

```{r}



for(i in 1:nrow(song_data5)){

  u0 <- U0_large[song_data5$participant[i]]
  u1 <- U1_large[song_data5$participant[i]]
  w0 <- W0[which(unique(song_data5$song) == song_data5$song[i])]
  x <- ifelse(song_data5$genre[i] == "rock", 0.5, -0.5)

  song_data5$Y[i] <- b0  + u0  + w0  + (b1 + u1) * x + error_large[i]
}

```

and fit the model

```{r}
song_model_7 <- lmer(Y ~ genre + (1+genre|participant) + (1|song), data = song_data5)
summary(song_model_7)
```

The estimates are closer to what they are supposed to be now.
Note that, however, even in this large sample, if we repeated the simulation with another seed, the estimates will still widely vary.
Lets try to change the seed, and resample random effect values

```{r}
set.seed(4356) # use other simulation seed
U0_large <- rnorm(n_participants_large, 0, sd_u0)
U1_large <- rnorm(n_participants_large, 0, sd_u1)
W0_large <- rnorm(length(unique(song_data5$song)), 0, sd_w0) 
error_large <- rnorm(nrow(song_data5), 0, sd_error)
```

and resimulate the DV

```{r}

for(i in 1:nrow(song_data5)){

  u0 <- U0_large[song_data5$participant[i]]
  u1 <- U1_large[song_data5$participant[i]]
  w0 <- W0[which(unique(song_data5$song) == song_data5$song[i])]
  x <- ifelse(song_data5$genre[i] == "rock", 1, 0)

  song_data5$Y[i] <- b0  + u0  + w0  + (b1 + u1) * x + error_large[i]
}

```

and fit the model again

```{r}
song_model_8 <- lmer(Y ~ genre + (1+genre|participant) + (1|song), data = song_data5)
summary(song_model_8)
```

Now the estimate is again closer to 4.
This is not a bug but a feature of mixed-effects model simulations.
We assume that we only tested our hypothesis on a *random* subset of people from the population and other people might behave differently.
Similarly, we did not use *all* pop and rock songs out there, so the results depend on the idiosyncratic sample of songs that we used.
By simulating random effects, we can see what influence this has on the results.
In a power simulation, these factors will thus rightfully influence the power we get, as individual simulations acknowledge the uncertainty that we have about the sample of songs and participants that we might end up with.

# Part III - 👍: Adding random correlations

Random effects terms can also be correlated: For instance, has a high overall liking of music across both genres (𝛽_0+𝑢_0 close to 100), this would indicate that they cannot dislike either music genre much or else they would not be able to have an average liking of close to 100 across genres.
If they liked one music type (e.g. pop) with only 60 points, to have an average liking of e.g. 95, they would have to like rock music at 130, because (60+130)/2 = 95.
However, 130 is not a possible value on our scale and therefore this can’t be the case.
As a consequence, any person with high average liking across genres, is unlikely to have large values of 𝑢_1 .
In other terms, across people the values of 𝑢_0 and 𝑢_1 will be negatively correlated.

To simulate this, we need to make sure that across people, the values of 𝑢_0 and 𝑢_1 are negatively correlated.

First let's have a look at where the correlations show up in the model output:

```{r}
summary(song_model_8)
```

We can see that next to the estimate for the random effect of participant:genrerock, there is a correlation estimate of -0.03.
This is the correlation between the random intercept and the random slope for genre rock.
That it is so small makes sense, as we did not simulate any correlation between the random effects.

To simulate this, we can use the `MASS` package to simulate correlated random effects.
For this, we will calculate the covariance matrix of the random effects and then use the `mvrnorm` function to simulate the random effects.
The covariance matrix is basically the random effect sizes that we put in before, expressed in terms of variances and covariance instead of standard deviations.
For instance before we set sd_u0 (the random intercept term for participants) to 7 and sd_01 (the random slope term for genre effects over participants) to 3.5.
Assuming a correlation of -0.2 between the random effects, the covariance can be calculated as: 7*3.5*-0.2 = -4.9

To create a matrix `sigma_u01` containing this information we can run:

```{r}
library(MASS)
corr_u01 <- -0.2 # this will be the correlation between the random effects
sigma_u01 <- matrix(c(sd_u0^2, sd_u0*sd_u1*corr_u01, sd_u0*sd_u1*corr_u01, sd_u1^2), ncol = 2)
print(sigma_u01)
```

We see that now this contains a 2x2 matrix with the variance of sd_u0 (7\^2 = 49) and next to it the covariance 7*3.5*-0.2 = -4.9.
In the second row, we again have the covariance, followed by the variance of sd_u1 (3.5\^2 = 12.25).

Now we can simulate the random effects using this covariance matrix.

```{r}
set.seed(123)
U01 <- mvrnorm(n_participants_large,c(0,0),sigma_u01)
print(U01)

cor(U01)
```

You can read this as follows:

create a multivariate normal distribution (i.e. a data-set containing values of two correlated normal distributions), both of which have mean 0 (as we are simulating random effects as sd again) by using the covariance matrix sigma_u01.

We see that for each of our 100 participants, we have a value for the random intercept and a value for the random slope that are correlated with -0.2275207, which is close to what we wanted.

Now we can simulate the data again.

```{r}
song_data6 <- song_data_large

U0_correlated <- U01[,1] # as U0 we take the first column of the U01 matrix, as this contains the random intercept terms
U1_correlated <- U01[,2] # as U1 we take the second column of the U01 matrix, as this contains the random slope terms


for(i in 1:nrow(song_data6)){

  u0 <- U0_correlated[song_data6$participant[i]]
  u1 <- U1_correlated[song_data6$participant[i]]
  w0 <- W0[which(unique(song_data6$song) == song_data6$song[i])]
  x <- ifelse(song_data6$genre[i] == "rock", 0.5, -0.5)

  song_data6$Y[i] <- b0  + u0  + w0  + (b1 + u1) * x + error_large[i]
}
```

```{r}
song_model_9 <- lmer(Y ~ genre + (1+genre|participant) + (1|song), data = song_data6)
summary(song_model_9)
```

We can see, that now our estimated correlation is close to the correlation that we simulated.

# Part III ❤️ : How to come up with parameter values

In the above, we basically came up with parameter values out of thin air and just added all of the random terms on top of each other.
This is usually not what we would want to do.
In fact, as we already saw earlier, compared to a fixed-effects-only linear model (lm) the advantage of a mixed-model (lmer) is that can partition the residual variance (i.e. unexplained variance) into different sources of variance, which will of course *shrink* the overall variance.
In the steps above, we never changed the value of the error term (it was always kept at 10).
However, usually we would expect the total error to *decrease* as we add random effects, as some of the noise that appears to be random at first, will turn out to be attributable to participants (as in the random intercept term) or idiosyncratic liking differences in genres across participants (the random slope term) or idiosyncratic influences of songs.

Thus, instead of coming up with numbers for all of them, we can think about how much of the overall noise (the residual variance) will probably be attributable to participants, genres, and songs.

For instance, in our music example, we might think that just by factors we do not know, likings will vary strongly

Instead, by quickly skipping through some playlists listed on <https://musicstax.com/> we can see that a quick search for rock songs already shows a spotify popularity rating between 32% and 80%.
So as a rough starting point, lets say that average likings for songs altogether are around 50 points and that the error variance should be around 25 points, meaning that for any single song, the error will cause the scores to vary widely between 0 and 100.
So we set:

```{r}
b0 <- 50
total_error <- 25
```

Next let's assume that 25% of that noise is caused by people's overall differences in music liking:

```{r}
sd_u0 <- 0.25 * total_error
```

Next, lets assume that another 5% of the error is caused by people's idiosyncratic differences in how much they like rock over pop songs:

```{r}
sd_u1 <- 0.05 * total_error
```

Finally, lets assume that 25% of the error is caused by idiosyncratic differences in how much people like individual songs:

```{r}
sd_w0 <- 0.25 * total_error
```

Of course, in total these numbers should be lower than 1, because we cannot partition more variance than we have in the error term.

That leaves us with the new residual noise:

```{r}
sd_error <- sqrt(total_error - sd_u0 - sd_u1 - sd_w0)
```

Now we can simulate the data again.

```{r}
song_data7 <- song_data_large
set.seed(123)
U0 <- rnorm(n_participants_large, 0, sd_u0)
U1 <- rnorm(n_participants_large, 0, sd_u1)
W0 <- rnorm(length(unique(song_data7$song)), 0, sd_w0)
error <- rnorm(nrow(song_data7), 0, sd_error)

for(i in 1:nrow(song_data7)){

  u0 <- U0[song_data7$participant[i]]
  u1 <- U1[song_data7$participant[i]]
  w0 <- W0[which(unique(song_data5$song) == song_data7$song[i])]
  x <- ifelse(song_data7$genre[i] == "rock", 1, 0)

  song_data7$Y[i] <- b0  + u0  + w0  + (b1 + u1) * x + error[i]
}
```

and fit the model again

```{r}
song_model_10 <- lmer(Y ~ genre + (1+genre|participant) + (1|song), data = song_data7)
summary(song_model_10)
```

Of course, this is still pretty much eye-balling, but if you have more informed domain knowledge, which you might have if you are running an experiment in an existing research line, this can be a good starting point to think about how much of the variance you expect to be attributable to different sources.

# Part III 🎉 : Simulating nested data

It could be possible that the liking that people show for music also depend on what country they live in as music could be more or less important in a culture.
To address this, we could add a random term that identifies different countries.
For instance, we could assume that our 100 participants in this simulation come from 5 different countries (I am not qualified to make any actual statements about which countries would show which effect and do not want to imply things only for the sake of an example so I will just call the countries Country A - Country E).
We can also assume that the countries are of different size so that they will not be equally represented by the 500 people in our sample.

```{r}

song_data8 <- song_data_large
set.seed(123)
n_participants <- 500
country_list <- sample(c("A", "B", "C", "D", "E"),size = length(unique(song_data8$participant)), prob = c(0.25, 0.15, 0.05, 0.35, 0.2), replace = T)

for(i in 1:nrow(song_data8)){
  song_data8$country[i] <- country_list[song_data8$participant[i]]
}
View(song_data8)
```

Lets assume that 10% of the variance that we previously thought was attributed to participants is actually attributed to countries.

```{r}
sd_u0 <- 7
sd_cu0 <- 0.1 * sd_u0
sd_u0 <- 7*0.9
```

Now we can simulate the data again.

```{r}
set.seed(123)
U0 <- rnorm(n_participants, 0, sd_u0)
CU0 <- rnorm(length(unique(song_data8$country)), 0, sd_cu0)
U1 <- rnorm(n_participants, 0, sd_u1)
W0 <- rnorm(length(unique(song_data8$song)), 0, sd_w0)
error <- rnorm(nrow(song_data8), 0, sd_error)

for(i in 1:nrow(song_data8)){

  u0 <- U0[song_data8$participant[i]]
  cu0 <- CU0[which(unique(song_data8$country) == song_data8$country[i])]
  u1 <- U1[song_data8$participant[i]]
  w0 <- W0[which(unique(song_data8$song) == song_data8$song[i])]
  x <- ifelse(song_data8$genre[i] == "rock", 1, 0)

  song_data8$Y[i] <- b0  + u0 + cu0 + w0  + (b1 + u1) * x + error[i]
}
```

and fit the model again

```{r}
song_model_11 <- lmer(Y ~ genre + (1+genre|participant) + (1|song) + (1|country), data = song_data8)
summary(song_model_11)
```

We can see that the model shows a singularity warning in this case which might be due to the fact that A) the random intercept term for country is very small compared to the other terms and B) the random correlation for the country intercept and slope is estimated based on only 5 pairs of observations (1 for each country), which is obviously very little information.

# Part III 😮 : Simulating binomial responses

In many experiments, our outcomes are actually not rating scales, but might be binary.
For example, we could have an experiment that asks participants to classify words as "positive" or "negative" based on their emotional content.

In this case, we can simulate the data in a similar way as before, but instead of simulating a continuous dependent variable, we can simulate a binary dependent variable.

For instance, we could simulate that people are more likely to classify words as positive if they are presented in green vs. red color for a very brief period of time and giving very little time to think about it.
We will code the DV in terms of it correctness, where 1 indicates a correct classification and 0 an incorrect classification.
We assume that people are quite good at this, and that the overall probability of a correct classification is 0.8.

```{r}
b0 <- 0.8
```

The effect of color is assumed to rather small with 0.05, meaning that people are 5% more likely to classify words as positive if they are presented in green vs. red color.

```{r}
b1 <- 0.05
```

Moreover, we assume that people differ in their overall ability to classify words correctly, with a standard deviation of 0.1 and that people differ in how much the color effect influences their classification, with a standard deviation of 0.025.
Finally, as some words might be more difficult to classify than others, we assume that there is an error term that is normally distributed with a standard deviation of 0.1.
In this case, we can also add a random slope over words, as the color effect might be stronger for some words than for others.
We can assume this effect to be small with a standard deviation of 0.01.
Finally, we will assume an residual error term of 0.2.

```{r}
sd_u0 <- 0.1
sd_u1 <- 0.025
sd_w0 <- 0.2
sd_w1 <- 0.05
sd_error <- 0.01
```

We will use the generate_design function again and change the variable names to match our new simulation.

```{r}
n_participants <- 50
n_words <- 50
n_colors <- 2

generate_design_words <- function(n_participants, n_colors, n_words){
  
  
  design_matrix <- expand.grid(participant = 1:n_participants, color = 1:n_colors, word = 1:n_words) # adding song to expand.grid
  design_matrix$color <- ifelse(design_matrix$color ==1, "green", "red")
  design_matrix$color <- factor(design_matrix$color)
  # make green the reference level, so it gets 1 in the contrast matrix and red gets -1
  design_matrix$color <- relevel(design_matrix$color, ref = "green")
  contrasts(design_matrix$color) <- contr.sum(2)
  design_matrix$word <- paste0("word", "_", design_matrix$word) 
  return(design_matrix)
}

word_data <- generate_design_words(n_participants = n_participants, n_colors = n_colors, n_words = n_words)

View(word_data)

```

We can see, that in this data, we have 100 participants, 2 colors, and 100 words, where each word is presented in both colors.

Now we can simulate the data.

```{r}
set.seed(123)
U0 <- rnorm(n_participants, 0, sd_u0)
U1 <- rnorm(n_participants, 0, sd_u1)
W0 <- rnorm(n_words, 0, sd_w0)
W1 <- rnorm(n_words, 0, sd_w1)
error <- rnorm(nrow(word_data), 0, sd_error)

word_data1 <- word_data

for(i in 1:nrow(word_data1)){
  u0 <- U0[word_data1$participant[i]]
  u1 <- U1[word_data1$participant[i]]
  w0 <- W0[which(unique(word_data1$word) == word_data1$word[i])]
  w1 <- W1[which(unique(word_data1$word) == word_data1$word[i])]
  x <- ifelse(word_data1$color[i] == "green", 0.5, -0.5)
  prob <- b0 + u0 + w0 + (b1 + u1 + w1) * x + error[i]
  prob <- ifelse(prob > 1, 1, prob)
  prob <- ifelse(prob < 0, 0, prob)

  word_data1$Y[i] <- rbinom(1, 1, prob)

}
```

and again fit our model - this time, we have to set the family to binomial.

```{r}
word_model <- glmer(Y ~ color + (1+color|participant) + (1+color|word), data = word_data1, family = binomial)
summary(word_model)
```

The numbers in this summary are more difficult to compare to what we simulated as they are in terms of log-odds.
We can use the emmeans package to get the probabilities for each level of the color variable.

```{r}
emmeans(word_model, "color", type = "response")
```
